# CUDA

## Project 1 
This  project  is  about  computing  spatial  distance  histogram  (SDH)  of  a  collection  of  3D  points.The SDH problem can be formally described as follows:  given the coordinates ofNparticles (e.g.,atoms, stars, moving objects in different applications) and a user-defined distancew, we need tocompute the number of particle-to-particle distances falling into a series of ranges (named buckets)of  widthw:  [0, w),[w,2w), . . . ,[(l−1)w, lw].   Essentially,  the  SDH  provides  an  ordered  list  ofnon-negative integersH= (h0, h1, . . . , hl−1), where eachhi(0≤i < l) is the number of distancesfalling into the bucket [iw,(i+ 1)w).  Clearly, the bucket widthwis a key parameter of an SDH tobe computed.2  Getting StartedFor you to get started, we have written a C program to compute SDH. The attached fileSDH.cushows a sample program for CPUs and serves as the starting point of your coding in this project.Interesting thing is that you can still compile and run it under the CUDA environment.  Specifically,you can type the following command to compile it.nvcc SDH.cu -o SDHTo run the code, you add the following line to your testing script file:./SDH 10000 500.0Note that the executable takes two arguments, the first one is the total number of data points andthe second one is the bucket width (w) in the histogram to be computed.  We strongly suggest youcompile and run this piece of code before you start coding.3  Tasks to PerformYou have to write a CUDA program to implement the same functionality as shown in the CPUprogram.  Both CUDA kernel function and CPU function results should be displayed as output.Write a CUDA kernel that computes the distance counts in all buckets of the SDH, by makingeach thread work on one input data point.  After all the points are processed, the resulting SDHshould be copied back to a host data structure.1.  Transfer the input data array (i.e., atomlist as shown in the sample code) onto GPU deviceusing CUDA functions.2.  Write a kernel function to compute the distance between one point to all other points andupdate the histogram accordingly.  Note that between any two points there should be onlyone distance counted.1
3.  Copy the final histogram from the GPU back to the host side and output them in the sameformat as we did for the CPU results.4.  Compare this histogram with the one you computed using CPU bucket by bucket.  Outputany differences between them - this should be done by printing out a histogram with the samenumber of buckets as the original ones except each bucket contains the difference in countsof the corresponding buckets.Note that the output of your program should only contain the following: 
(1) the two histograms,one computed by CPU only and the other computed by your GPU kernel. 
(2) any difference youfound between these two histograms.4  EnvironmentAll projects will be tested on CUDA 7.5 on one machine of the C4 lab.  If you prefer to work on yourown computer, make sure your project can be executed on the GPU card of a C4 lab computer.5  Instructions to Submit ProjectYou should submit one .cu file with your implementation.  For this project,  we suggest you justadd code to the file named SDH.cu we provided.  Rename the file asproj1-xxx.cu, wherexxxisyour USF NetID. Submit the file only via assignment link in Canvas.  E-mail or any other form ofsubmission will not be graded.  Once you submit your file to Canvas, try to download the submittedfile and open it in your machine to make sure no data transmission problems occurred.  For that,we suggest you finish the project and submit the file at lease one hour before the deadline.6  Rules for GradingFollowing are some of the rules that you should keep in mind while submitting the project.  Theproject will be graded by a set of test cases we run against your code.•All programs that fail to compile will get zero points.  We expect your submission be compiledby running the simple line of command shown above.•If, after the submission deadline, any changes are to be made to make the main code work,they should be done within 3 lines of code.  This will incur a 30% penalty.•Program should run for different numbers of CUDA blocks and threads.  Use of any tricks tomake it run on one thread or only on CPU will result in zero points.  However, performanceof your GPU code is not considered in grading

## Project 2 
GPU Project 2:  A Better Version of SDH Computing Program Course: CIS6930 Massive Parallel Computing 
OverviewIn this project you will measure the performance of GPU memory system,  perform different ex-periments,  and write a report on your findings.  The problem in hand is:  computing the spatialdistance histogram of a set of points.  This is the same problem that you worked on in Project 1.The main objective of this project is to write very efficient CUDA code,  unlike in Project 1where performance was not a grading criterion.  You have the freedom to choose any techniquesyou  learned  from  class  to  improve  your  program,  these  include  (but  are  not  limited  to):  usingshared memory to hold input data, manipulating input data layout for coalesced memory access,managing thread workload to reduce code divergence, atomic operations, private copies of output,and shuffle instructions.  You can use a combination of different techniques, and the code will begraded based on your rank in a class-wise contest, in which we will use a set of different datasets totest your code.  Therefore, your goal is to try all you can think of to optimize your program.  Thatsaid, we will make your job easier by introducing the following paper published by our group.Napath Pitaksirianan, Zhila Nouri, and Yi-Cheng Tu.  “Efficient 2-Body Statistics Computationon  GPUs:  Parallelization  &  Beyond”.  Proceedings of 45th International Conference on ParallelProcessing, pp.  380-385., August 2016.The  paper  describes  a  series  of  techniques  for  achieving  high  performance  in  dealing  with  agroup of problems that share similar computing patterns as the SDH problem.2  Tasks to PerformWrite a CUDA program to implement the same functionalities as required in Project 1, performdifferent  experiments,  and  write  a  short  report  about  your  project.   The  CUDA  kernel  functionresults and running time of the kernel(s) should be displayed as output.  Thus, your main task it towrite an efficient CUDA kernel to compute the SDH. In addition, your program should also includethe following features.Input/Output of Your Program:You have to modify the program to take a different numberof command line arguments from what you did in Project 1.  Your program should take care of bador missing inputs and throw appropriate errors in a reasonable way.  In particular, here is what weexpect in launching your program:./proj2 {#of_samples} {bucket_width} {block_size}1
whereproj2is assumed to be the executable after compiling your project.  The first two argumentsare the same as in Project 1, while the last one is the number of threads within each block yourCUDA kernel should be launched.The output of your program should print out the SDH you computed as in Project 1.  Followingthe SDH, you should add a line to report the performance of your kernel, it should look like thefollowing sample.******** Total Running Time of Kernel = 2.0043 sec *******Please read Section 3 for details of measuring kernel running time.Project ReportWrite a report to explain clearly how you implemented the GPU kernels, witha focus on what techniques you used to optimize performance.3  Measuring Running TimeThe running time of the CPU implementation can be measured using different time functions avail-able in the C programming libraries.  One such function useful in measuring time isgettimeofday.Another  is  therdtscinstruction  supported  on  Pentium  CPUs.   You  can  also  use  theclock()function to record the time.  However, these functions cannot be used to measure the running timeof  GPU  kernels.   There  are  special  event  functions  that  are  used  to  record  the  running  time  ofkernels.  Following is an example to record running time of a kernel.1:  cudaEvent_t start, stop;2:  cudaEventCreate(&start);3:  cudaEventCreate(&stop);4:  cudaEventRecord( start, 0 );5:  /* Your Kernel call goes here */6:  cudaEventRecord( stop, 0 );7:  cudaEventSynchronize( stop );8:  float elapsedTime;9:  cudaEventElapsedTime( &elapsedTime, start, stop );10: printf( "Time to generate: %0.5f ms\n", elapsedTime );11: cudaEventDestroy( start );12: cudaEventDestroy( stop );Aneventin CUDA is essentially a GPU time stamp that is recorded at a user specified pointin time.  The API is relatively easy to use, since taking a time stamp consists of just two steps:creating  an  event  and  subsequently  recording  an  event.   For  example,  at  the  beginning  of  somesequence of code, we instruct the CUDA runtime to make a record of the current time.  We do soby creating and then recording the event (lines 2−4).  The exact nature of second argument in line4 is unimportant for our purposes right now (use 0 always).To time a block of code, we will want to create both a start event and a stop event.  We willrecord the CUDA time when we tell it to do some work on the GPU and then record the time againwhen we’ve stopped.2
Unfortunately, there is still a problem with timing GPU code in this way.  The fix will requireonly one line of code but will require some explanation.  The trickiest part of using events arises asa consequence of the fact that some of the calls we make in CUDA C are actually asynchronous.For example,  when we launch the kernel in line 5,  the GPU begins executing our code,  but theCPU continues executing the next line of our program before the GPU finishes.  This is excellentfrom a performance standpoint because it means we can be computing something on the GPU andCPU at the same time, but conceptually it makes timing tricky.You should imagine calls tocudaEventRecord()as an instruction to record the current timebeing  placed  into  the  GPU’s  pending  queue  of  work.   As  a  result,  our  event  won’t  actually  berecorded until the GPU finishes everything prior to the call tocudaEventRecord().  In terms ofhaving our stop event measure the correct time,  this is precisely what we want.  But we cannotsafely read the value of the stop event until the GPU has completed its prior work and recordedthe stop event.  Fortunately, we have a way to instruct the CPU to synchronize on an event, theevent API functioncudaEventSynchronize().Now, we have instructed the runtime to block further instruction (line 7) until the GPU hasreached the stop event.  When the call tocudaEventSynchronize()returns, we know that all GPUwork before the stop event has completed, so it is safe to read the time stamp recorded in stop.  It isworth noting that because CUDA events get implemented directly on the GPU, they are unsuitablefor timing mixtures of device and host code.  That is, you will get unreliable results if you attemptto use CUDA events to time more than kernel executions and memory copies involving the device.The functioncudaEventElapsedTime()is a utility that computes the elapsed time between twopreviously recorded events.  The time in milliseconds elapsed between the two events is returned inthe first argument, the address of a floating-point variable.The call tocudaEventDestroy()needs to be made when we’re finished using an event createdwithcudaEventCreate().  This is identical to callingfree()on memory previously allocated withmalloc(),  so  we  needn’t  stress  how  important  it  is  to  match  everycudaEventCreate()with  acudaEventDestroy().The content about measuring time on GPUs explained in this section is taken from the followingreference book:Jason Sanders and Edward Kandrot.  “CUDA  by  Example:  An  Introduction  to  General-PurposeGPU Programming”.  Addison-Wesley, 2011.
